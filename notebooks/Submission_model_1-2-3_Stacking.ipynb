{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87616bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Installazioni necessarie (per Kaggle) ---\n",
    "# !pip install -q xgboost catboost lightgbm\n",
    "\n",
    "# --- Import di base ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Import Modelli e Utility ---\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Impostazioni ---\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clona il repository ---\n",
    "GIT_REPO_URL = \"https://github.com/Gabriele-mp/FDS-DITTO-DATI.git\"\n",
    "REPO_NAME = GIT_REPO_URL.split('/')[-1].replace('.git', '')\n",
    "\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    print(f\"Clonazione repository: {GIT_REPO_URL}...\")\n",
    "    !git clone -q {GIT_REPO_URL}\n",
    "else:\n",
    "    print(f\"Repository {REPO_NAME} giÃ  presente.\")\n",
    "\n",
    "# --- Aggiungi 'src' al path di sistema ---\n",
    "SRC_PATH = os.path.join(os.getcwd(), REPO_NAME, 'src')\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)\n",
    "    print(f\"Aggiunto '{SRC_PATH}' a sys.path\")\n",
    "\n",
    "print(\"Repository e path pronti.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367de474",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # --- Import per Modello 1 ---\n",
    "    print(\"Importazione funzioni per Modello 1 (Stacking)...\")\n",
    "    from data_processing import load_and_clean_data\n",
    "    from feature_builder import (\n",
    "        extract_features_v8,\n",
    "        extract_features_v21,\n",
    "        extract_moveset_features,\n",
    "        extract_features_CRITICAL_MISSING\n",
    "    )\n",
    "    from train_utils import build_feature_dataframe\n",
    "\n",
    "    # --- Import per Modello 2 ---\n",
    "    print(\"Importazione funzioni per Modello 2...\")\n",
    "    from feature_builder_Model2 import (\n",
    "        extract_features_v8 as extract_v8_M2,\n",
    "        extract_features_v20 as extract_v20_M2,\n",
    "        build_feature_dataframe as build_df_M2\n",
    "    )\n",
    "    \n",
    "    # --- Import per Modello 3 ---\n",
    "    print(\"Importazione funzioni per Modello 3...\")\n",
    "    from config_Model3 import *\n",
    "    from feature_builder_Model3 import (\n",
    "        extract_features_v8 as extract_v8_M3,\n",
    "        extract_features_v19 as extract_v19_M3,\n",
    "        extract_features_v20 as extract_v20_M3,\n",
    "        build_feature_dataframe as build_df_M3\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Funzioni importate con successo per tutti i modelli!\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Errore nell'import: {e}\")\n",
    "    print(\"Verifica che il path nella Cella 2 sia corretto e che i file .py in 'src/' siano aggiornati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7493a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELLA 4 (AGGIORNATA): Configurazione e Caricamento Dati\n",
    "# ===================================================================\n",
    "\n",
    "# --- Configurazione Globale ---\n",
    "COMPETITION_NAME = 'fds-pokemon-battles-prediction-2025'\n",
    "DATA_PATH = os.path.join('../input', COMPETITION_NAME)\n",
    "SEED = 123\n",
    "N_SPLITS = 5\n",
    "\n",
    "# Definisci KFold (usato da entrambi i modelli)\n",
    "kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# --- Caricamento Dati di Training ---\n",
    "print(\"Caricamento e pulizia dati di TRAINING...\")\n",
    "train_file_path = os.path.join(DATA_PATH, 'train.jsonl')\n",
    "# Usiamo la TUA funzione di pulizia (load_and_clean_data)\n",
    "df_train_shuffled = load_and_clean_data(train_file_path, seed=SEED, is_train=True)\n",
    "y_train = df_train_shuffled['player_won'].astype(int) # Target, usato da tutti\n",
    "print(f\"Dati di training pronti: {df_train_shuffled.shape}\")\n",
    "\n",
    "# --- Caricamento Dati di Test ---\n",
    "print(\"\\nCaricamento dati di TEST...\")\n",
    "test_file_path = os.path.join(DATA_PATH, 'test.jsonl')\n",
    "df_test_raw = load_and_clean_data(test_file_path, is_train=False)\n",
    "battle_ids = df_test_raw['battle_id'] # ID per la submission, usati da tutti\n",
    "print(f\"Dati di test pronti: {df_test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd64c3",
   "metadata": {},
   "source": [
    "## ðŸ¤– Submission 1: 3-Model Stacking Ensemble\n",
    "\n",
    "This submission is generated by a **Stacking Ensemble** designed to maximize accuracy by combining the predictions of 3 heterogeneous base models.\n",
    "\n",
    "**Architecture:**\n",
    "* **Level 0 (Feature Sets):**\n",
    "    * **Set 1 (v8):** A compact set of ~30 features (used by LR).\n",
    "    * **Set 2 (v21):** A broad set of ~100+ features (used by LGBM).\n",
    "    * **Set 3 (Mega-Set):** A \"supreme\" set of 145 features (v21 + Moveset + Critical), reduced to 120 via feature selection (used by the second LGBM).\n",
    "* **Level 1 (Base Models):**\n",
    "    1.  `lr_v8`: **Optimized LogisticRegression** (on Set v8).\n",
    "    2.  `lgbm_v21`: **LightGBM** (on Set v21).\n",
    "    3.  `lgbm_mega`: **LightGBM** (on the selected Mega-Set).\n",
    "* **Level 2 (Meta-Model):**\n",
    "    * The Out-of-Fold (OOF) predictions from the 3 base models become the *meta-features* used to train a final **LogisticRegression**, which acts as a \"judge\" to weigh and combine the results.\n",
    "\n",
    "**Expected CV (Cross-Validation) Score:** 0.8528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Inizio Pipeline di Training ---\")\n",
    "\n",
    "# ===================================================================\n",
    "# 1. GENERAZIONE FEATURE SET DI TRAINING\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 1/6] Generazione Feature Set di Training...\")\n",
    "\n",
    "# Set 1 (LR)\n",
    "X_train_v8, y_train = build_feature_dataframe(df_train_shuffled, extract_features_v8, is_test_set=False)\n",
    "# Set 2 (LGBM)\n",
    "X_train_v21, _ = build_feature_dataframe(df_train_shuffled, extract_features_v21, is_test_set=False)\n",
    "# Set 3 (per Mega-Set)\n",
    "X_train_moveset, _ = build_feature_dataframe(df_train_shuffled, extract_moveset_features, is_test_set=False)\n",
    "X_train_ultimate, _ = build_feature_dataframe(df_train_shuffled, extract_features_CRITICAL_MISSING, is_test_set=False)\n",
    "\n",
    "# ===================================================================\n",
    "# 2. CREAZIONE E PULIZIA \"MEGA-SET\"\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 2/6] Creazione e Pulizia 'Mega-Set'...\")\n",
    "\n",
    "X_train_v21_safe = X_train_v21.reset_index(drop=True)\n",
    "X_train_moveset_safe = X_train_moveset.reset_index(drop=True)\n",
    "X_train_ultimate_safe = X_train_ultimate.reset_index(drop=True)\n",
    "\n",
    "X_train_MEGASUPERSET = pd.concat([X_train_v21_safe, X_train_moveset_safe, X_train_ultimate_safe], axis=1)\n",
    "X_train_MEGASUPERSET = X_train_MEGASUPERSET.loc[:,~X_train_MEGASUPERSET.columns.duplicated()]\n",
    "print(f\"Shape Mega-Set (grezzo): {X_train_MEGASUPERSET.shape}\")\n",
    "\n",
    "# Feature Selection\n",
    "lgbm_selector = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31, random_state=SEED, verbose=-1)\n",
    "lgbm_selector.fit(X_train_MEGASUPERSET, y_train)\n",
    "importances = pd.Series(lgbm_selector.feature_importances_, index=X_train_MEGASUPERSET.columns)\n",
    "\n",
    "top_120_features = importances.nlargest(120).index\n",
    "X_train_MEGA_SELECTED = X_train_MEGASUPERSET[top_120_features]\n",
    "print(f\"Shape Mega-Set (pulito): {X_train_MEGA_SELECTED.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. OTTIMIZZAZIONE LR\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 3/6] Ottimizzazione Iperparametri (LR)...\")\n",
    "\n",
    "param_grid_lr = {'model__C': [1.0, 5.0, 10.0, 15.0, 20.0, 25.0]}\n",
    "model_lr_v8_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(penalty='l2', solver='saga', max_iter=5000, random_state=SEED))\n",
    "])\n",
    "grid_lr = GridSearchCV(estimator=model_lr_v8_pipeline, param_grid=param_grid_lr, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "grid_lr.fit(X_train_v8, y_train)\n",
    "lr_v8_OPTIMIZED = grid_lr.best_estimator_\n",
    "\n",
    "print(f\"LR Ottimizzato. Miglior Score: {grid_lr.best_score_:.4f}, Migliori Parametri: {grid_lr.best_params_}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 4. DEFINIZIONE MODELLI BASE\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 4/6] Definizione Modelli Base L1...\")\n",
    "base_models = {}\n",
    "\n",
    "# Modello 1 (LR Ottimizzato)\n",
    "base_models['lr_v8'] = (lr_v8_OPTIMIZED, X_train_v8)\n",
    "\n",
    "# Modello 2 (LGBM v21)\n",
    "lgbm_v21 = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31, random_state=SEED, verbose=-1)\n",
    "base_models['lgbm_v21'] = (lgbm_v21, X_train_v21)\n",
    "\n",
    "# Modello 3 (LGBM Mega)\n",
    "lgbm_mega = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31, random_state=SEED, verbose=-1)\n",
    "base_models['lgbm_mega'] = (lgbm_mega, X_train_MEGA_SELECTED)\n",
    "\n",
    "print(f\"Definiti {len(base_models)} modelli base: {list(base_models.keys())}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 5. ESECUZIONE STACKING (OOF)\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 5/6] Esecuzione Stacking (OOF)...\")\n",
    "\n",
    "X_meta_df = pd.DataFrame(np.zeros((len(y_train), len(base_models))), columns=base_models.keys())\n",
    "final_base_models = {} # Modelli addestrati su tutto il training set\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(y_train, y_train), total=N_SPLITS, desc=\"Folds\")):\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    for name, (model, X_data) in base_models.items():\n",
    "        X_train_fold = X_data.iloc[train_idx]\n",
    "        X_val_fold = X_data.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        X_meta_df.loc[val_idx, name] = model.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "print(\"Addestramento modelli base finali su tutti i dati...\")\n",
    "for name, (model, X_data) in tqdm(base_models.items(), desc=\"Modelli Finali L1\"):\n",
    "    final_base_models[name] = model.fit(X_data, y_train)\n",
    "\n",
    "print(\"âœ… Meta-Features (X_meta_df) create.\")\n",
    "\n",
    "# ===================================================================\n",
    "# 6. ADDESTRAMENTO META-MODELLO L2\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 6/6] Addestramento Meta-Modello L2...\")\n",
    "\n",
    "# Analisi e selezione (usiamo tutti e 3 i modelli come da analisi precedente)\n",
    "optimal_models = ['lr_v8', 'lgbm_v21', 'lgbm_mega']\n",
    "X_meta_df_optimal = X_meta_df[optimal_models].copy()\n",
    "\n",
    "# Addestra Meta-Modello Finale\n",
    "meta_model = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "final_ensemble_model = meta_model.fit(X_meta_df_optimal, y_train)\n",
    "\n",
    "# Stampa CV score\n",
    "final_cv_score = cross_val_score(meta_model, X_meta_df_optimal, y_train, cv=kfold, scoring='accuracy', n_jobs=-1).mean()\n",
    "\n",
    "print(f\"âœ… Pipeline di Training completata.\")\n",
    "print(f\"ðŸŽ¯ CV Score Finale (stimato): {final_cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22add2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Inizio Pipeline di Predizione (Test Set) ---\")\n",
    "\n",
    "# ===================================================================\n",
    "# 1. CARICAMENTO DATI DI TEST\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 1/3] Caricamento Dati di Test...\")\n",
    "test_file_path = os.path.join(DATA_PATH, 'test.jsonl')\n",
    "df_test_raw = load_and_clean_data(test_file_path, is_train=False)\n",
    "battle_ids = df_test_raw['battle_id']\n",
    "\n",
    "# ===================================================================\n",
    "# 2. GENERAZIONE FEATURE SET DI TEST\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 2/3] Generazione Feature Set di Test...\")\n",
    "\n",
    "# Set 1 (LR)\n",
    "X_test_v8, _ = build_feature_dataframe(df_test_raw, extract_features_v8, is_test_set=True)\n",
    "X_test_v8 = X_test_v8[X_train_v8.columns] # Allinea colonne\n",
    "\n",
    "# Set 2 (LGBM)\n",
    "X_test_v21, _ = build_feature_dataframe(df_test_raw, extract_features_v21, is_test_set=True)\n",
    "X_test_v21 = X_test_v21[X_train_v21.columns] # Allinea colonne\n",
    "\n",
    "# Set 3 (per Mega-Set)\n",
    "X_test_moveset, _ = build_feature_dataframe(df_test_raw, extract_moveset_features, is_test_set=True)\n",
    "X_test_moveset = X_test_moveset[X_train_moveset.columns] # Allinea colonne\n",
    "X_test_ultimate, _ = build_feature_dataframe(df_test_raw, extract_features_CRITICAL_MISSING, is_test_set=True)\n",
    "X_test_ultimate = X_test_ultimate[X_train_ultimate.columns] # Allinea colonne\n",
    "\n",
    "# Costruisci X_test_MEGA_SELECTED\n",
    "X_test_v21_safe = X_test_v21.reset_index(drop=True)\n",
    "X_test_moveset_safe = X_test_moveset.reset_index(drop=True)\n",
    "X_test_ultimate_safe = X_test_ultimate.reset_index(drop=True)\n",
    "\n",
    "X_test_MEGASUPERSET = pd.concat([X_test_v21_safe, X_test_moveset_safe, X_test_ultimate_safe], axis=1)\n",
    "X_test_MEGASUPERSET = X_test_MEGASUPERSET.loc[:,~X_test_MEGASUPERSET.columns.duplicated()]\n",
    "\n",
    "# Usa le top_120_features definite nella Cella 6\n",
    "X_test_MEGA_SELECTED = X_test_MEGASUPERSET[top_120_features]\n",
    "print(f\"Shape X_test_MEGA_SELECTED: {X_test_MEGA_SELECTED.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. GENERAZIONE SUBMISSION\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 3/3] Generazione Submission Finale...\")\n",
    "\n",
    "# Mappa per i dati di test\n",
    "test_data_map = {\n",
    "    'lr_v8': X_test_v8,\n",
    "    'lgbm_v21': X_test_v21,\n",
    "    'lgbm_mega': X_test_MEGA_SELECTED\n",
    "}\n",
    "\n",
    "# Crea Meta-Features di Test L2\n",
    "X_meta_test_df = pd.DataFrame(columns=optimal_models)\n",
    "for name, model in tqdm(final_base_models.items(), desc=\"Predizioni Test L1\"):\n",
    "    if name in optimal_models:\n",
    "        X_test_data = test_data_map.get(name)\n",
    "        X_meta_test_df[name] = model.predict_proba(X_test_data)[:, 1]\n",
    "\n",
    "# Predizione Finale L2\n",
    "final_predictions = final_ensemble_model.predict(X_meta_test_df[optimal_models])\n",
    "\n",
    "# --- Creazione File Submission ---\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': battle_ids,\n",
    "    'player_won': final_predictions.astype(int)\n",
    "})\n",
    "\n",
    "submission_filename = 'submission.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nâœ… File '{submission_filename}' creato con successo!\")\n",
    "print(f\"ðŸ“Š Predizioni: {len(final_predictions)}\")\n",
    "print(f\"ðŸ“ˆ Distribuzione (0 vs 1): {np.bincount(final_predictions)}\")\n",
    "print(f\"ðŸŽ¯ CV Score Atteso: {final_cv_score:.4f}\")\n",
    "\n",
    "display(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5876ab",
   "metadata": {},
   "source": [
    "## ðŸ¤– Submission 2: 2-Model Stacking Ensemble (LR + XGB)\n",
    "\n",
    "This submission is generated by a 2-model stacking ensemble, based on the `Model2` development pipeline.\n",
    "\n",
    "**Architecture:**\n",
    "* **Level 0 (Feature Sets):**\n",
    "    * **Set 1 (v8_M2):** The compact `v8` feature set (from `feature_builder_Model2.py`) used by the Logistic Regression.\n",
    "    * **Set 2 (v20_M2):** The larger `v20` feature set (from `feature_builder_Model2.py`) used by the XGBoost model.\n",
    "* **Level 1 (Base Models):**\n",
    "    1.  `lr_v8`: **LogisticRegression** (on Set v8_M2).\n",
    "    2.  `xgb_v20`: **XGBoost** (on Set v20_M2).\n",
    "* **Level 2 (Meta-Model):**\n",
    "    * The OOF predictions from these two models are used as meta-features to train a final **LogisticRegression** meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061801d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELLA 1 : PIPELINE MODELLO 2 \n",
    "# ===================================================================\n",
    "print(\"\\n--- Inizio Pipeline Modello 2: Stacking (LR+XGB) ---\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Generazione Feature (da 02_Feature_Engineering) ---\n",
    "    print(\"[M2 - Fase 1/5] Generazione Feature Set (v8, v20)...\")\n",
    "    \n",
    "    # Usiamo le funzioni _M2 importate con alias\n",
    "    # Il train set restituisce 2 valori (X, y) - QUESTO Ãˆ CORRETTO\n",
    "    X_train_v8_M2, _ = build_df_M2(df_train_shuffled, extract_v8_M2, is_test_set=False)\n",
    "    X_train_v20_M2, _ = build_df_M2(df_train_shuffled, extract_v20_M2, is_test_set=False)\n",
    "    \n",
    "    # === CORREZIONE QUI ===\n",
    "    # Il test set restituisce 1 valore (X) - Rimuoviamo il \", _\"\n",
    "    print(\"\\nGenerazione feature di test per M2...\")\n",
    "    X_test_v8_M2 = build_df_M2(df_test_raw, extract_v8_M2, is_test_set=True)\n",
    "    X_test_v20_M2 = build_df_M2(df_test_raw, extract_v20_M2, is_test_set=True)\n",
    "    # === FINE CORREZIONE ===\n",
    "    \n",
    "    # Allinea colonne\n",
    "    X_test_v8_M2 = X_test_v8_M2[X_train_v8_M2.columns] \n",
    "    X_test_v20_M2 = X_test_v20_M2[X_train_v20_M2.columns]\n",
    "    \n",
    "    print(f\"Feature M2 (Train): v8({X_train_v8_M2.shape}), v20({X_train_v20_M2.shape})\")\n",
    "    print(f\"Feature M2 (Test): v8({X_test_v8_M2.shape}), v20({X_test_v20_M2.shape})\")\n",
    "\n",
    "    # --- 2. Definizione Modelli Base (L0) (da 02_All_Base_Models_Training) ---\n",
    "    print(\"\\n[M2 - Fase 2/5] Definizione Modelli Base (L0)...\")\n",
    "    base_models_M2 = {}\n",
    "\n",
    "    # Modello LR (v8)\n",
    "    model_lr_v8_M2 = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(C=10.0, penalty='l2', solver='saga', max_iter=5000, random_state=SEED))\n",
    "    ])\n",
    "    base_models_M2['lr_v8'] = (model_lr_v8_M2, X_train_v8_M2)\n",
    "\n",
    "    # Modello XGB (v20)\n",
    "    model_xgb_v20_M2 = XGBClassifier(\n",
    "        colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200,\n",
    "        reg_lambda=5, subsample=0.7, objective='binary:logistic',\n",
    "        eval_metric='logloss', use_label_encoder=False, random_state=SEED\n",
    "    )\n",
    "    base_models_M2['xgb_v20'] = (model_xgb_v20_M2, X_train_v20_M2)\n",
    "    \n",
    "    optimal_models_M2 = ['lr_v8', 'xgb_v20']\n",
    "\n",
    "    # --- 3. Stacking (OOF) (da 02_All_Base_Models_Training) ---\n",
    "    print(\"\\n[M2 - Fase 3/5] Esecuzione Stacking (OOF)...\")\n",
    "    \n",
    "    # y_train Ã¨ giÃ  definito nella Cella 4\n",
    "    X_meta_train_M2 = pd.DataFrame(np.zeros((len(y_train), len(optimal_models_M2))), columns=optimal_models_M2)\n",
    "    final_base_models_M2 = {} \n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(y_train, y_train), total=N_SPLITS, desc=\"[M2] Folds\")):\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        for name, (model, X_data) in base_models_M2.items():\n",
    "            if name in optimal_models_M2:\n",
    "                X_train_fold = X_data.iloc[train_idx]\n",
    "                X_val_fold = X_data.iloc[val_idx]\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                X_meta_train_M2.loc[val_idx, name] = model.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "    print(\"Addestramento modelli base L0 finali (su tutti i dati)...\")\n",
    "    for name, (model, X_data) in tqdm(base_models_M2.items(), desc=\"[M2] Modelli Finali L0\"):\n",
    "        if name in optimal_models_M2:\n",
    "            final_base_models_M2[name] = model.fit(X_data, y_train)\n",
    "\n",
    "    # --- 4. Addestramento Meta-Modello (L1) (da 02_Ensemble_Submission) ---\n",
    "    print(\"\\n[M2 - Fase 4/5] Addestramento Meta-Modello (L1)...\")\n",
    "    \n",
    "    final_ensemble_model_M2 = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "    final_ensemble_model_M2.fit(X_meta_train_M2, y_train)\n",
    "    print(\"Meta-Modello (L1) addestrato.\")\n",
    "\n",
    "    # --- 5. Predizione e Salvataggio (da 02_Ensemble_Submission) ---\n",
    "    print(\"\\n[M2 - Fase 5/5] Generazione Submission Modello 2...\")\n",
    "    \n",
    "    X_meta_test_M2 = pd.DataFrame()\n",
    "    X_meta_test_M2['lr_v8'] = final_base_models_M2['lr_v8'].predict_proba(X_test_v8_M2)[:, 1]\n",
    "    X_meta_test_M2['xgb_v20'] = final_base_models_M2['xgb_v20'].predict_proba(X_test_v20_M2)[:, 1]\n",
    "\n",
    "    final_predictions_M2 = final_ensemble_model_M2.predict(X_meta_test_M2)\n",
    "\n",
    "    submission_df_2 = pd.DataFrame({\n",
    "        'battle_id': battle_ids,\n",
    "        'player_won': final_predictions_M2.astype(int)\n",
    "    })\n",
    "\n",
    "    submission_filename_2 = 'submission_model_2.csv'\n",
    "    submission_df_2.to_csv(submission_filename_2, index=False)\n",
    "\n",
    "    print(f\"âœ… File '{submission_filename_2}' creato con successo!\")\n",
    "    display(submission_df_2.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!!! ERRORE DURANTE LA PIPELINE DEL MODELLO 2 !!!\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf72a9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ðŸ¤– Submission 3: Multi-Model Ensemble with Advanced Feature Engineering\n",
    "\n",
    "This submission uses a **3-model ensemble** combining Logistic Regression, XGBoost, and Random Forest, each trained on specialized feature sets.\n",
    "\n",
    "**Architecture:**\n",
    "* **Feature Sets:**\n",
    "    * **v8_M3:** ~200 optimized features for Logistic Regression\n",
    "    * **v19_M3:** ~250 advanced features for Random Forest\n",
    "    * **v20_M3:** ~230 tree-optimized features for XGBoost\n",
    "* **Models:**\n",
    "    1. Logistic Regression (C=10, L2 regularization, StandardScaler)\n",
    "    2. XGBoost (300 trees, depth=6, lr=0.05)\n",
    "    3. Random Forest (200 trees, depth=15)\n",
    "* **Ensemble:** Weighted averaging (30% LR + 50% XGB + 20% RF)\n",
    "\n",
    "**Implementation:** Feature extraction via `feature_builder_Model3.py`, hyperparameters in `config_Model3.py`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CELLA 3: NUOVA CELLA CODE - SUBMISSION MODEL 3\n",
    "# ==============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"SUBMISSION 3: MODEL 3 - ADVANCED FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Usa il test data giÃ  caricato\n",
    "df_test = df_test_raw\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Genera features test\n",
    "X_test_v8_M3 = build_df_M3(df_test, extract_v8_M3, is_test_set=True)\n",
    "X_test_v19_M3 = build_df_M3(df_test, extract_v19_M3, is_test_set=True)\n",
    "X_test_v20_M3 = build_df_M3(df_test, extract_v20_M3, is_test_set=True)\n",
    "\n",
    "X_train_v8_M3, y_train = build_df_M3(df_train_shuffled, extract_v8_M3, is_test_set=False)\n",
    "X_train_v19_M3 = build_df_M3(df_train_shuffled, extract_v19_M3, is_test_set=False)\n",
    "X_train_v20_M3 = build_df_M3(df_train_shuffled, extract_v20_M3, is_test_set=False)\n",
    "\n",
    "#debug:\n",
    "print(f\"X_train_v8_M3 type: {type(X_train_v8_M3)}, shape: {X_train_v8_M3.shape if hasattr(X_train_v8_M3, 'shape') else 'N/A'}\")\n",
    "print(f\"X_train_v20_M3 type: {type(X_train_v20_M3)}, shape: {X_train_v20_M3.shape if hasattr(X_train_v20_M3, 'shape') else 'N/A'}\")\n",
    "\n",
    "# SE Ã¨ un DataFrame, converti:\n",
    "if isinstance(X_train_v20_M3, pd.DataFrame):\n",
    "    X_train_v20_M3 = X_train_v20_M3.values\n",
    "if isinstance(X_train_v19_M3, pd.DataFrame):\n",
    "    X_train_v19_M3 = X_train_v19_M3.values\n",
    "\n",
    "# Train Logistic Regression\n",
    "model_lr_m3 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(C=10.0, penalty='l2', solver='lbfgs', \n",
    "                                  max_iter=1000, random_state=SEED))\n",
    "])\n",
    "model_lr_m3.fit(X_train_v8_M3, y_train)\n",
    "\n",
    "# Train XGBoost\n",
    "model_xgb_m3 = XGBClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, \n",
    "    random_state=SEED, eval_metric='logloss'\n",
    ")\n",
    "model_xgb_m3.fit(X_train_v20_M3, y_train)\n",
    "\n",
    "# Train Random Forest\n",
    "model_rf_m3 = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=15, \n",
    "    min_samples_split=10, random_state=SEED\n",
    ")\n",
    "model_rf_m3.fit(X_train_v19_M3, y_train)\n",
    "\n",
    "# Predizioni\n",
    "pred_lr_m3 = model_lr_m3.predict_proba(X_test_v8_M3)[:, 1]\n",
    "pred_xgb_m3 = model_xgb_m3.predict_proba(X_test_v20_M3)[:, 1]\n",
    "pred_rf_m3 = model_rf_m3.predict_proba(X_test_v19_M3)[:, 1]\n",
    "\n",
    "# Ensemble\n",
    "pred_ensemble_m3 = 0.3 * pred_lr_m3 + 0.5 * pred_xgb_m3 + 0.2 * pred_rf_m3\n",
    "pred_final_m3 = (pred_ensemble_m3 >= 0.5).astype(int)\n",
    "\n",
    "# Save\n",
    "submission_m3 = pd.DataFrame({'id': df_test['id'], 'winner': pred_final_m3})\n",
    "submission_m3.to_csv('submission_3.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Submission salvata | Distribuzione: {Counter(pred_final_m3)}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

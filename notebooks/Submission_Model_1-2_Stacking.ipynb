{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Installazioni necessarie (per Kaggle) ---\n",
    "# !pip install -q xgboost catboost lightgbm\n",
    "\n",
    "# --- Import di base ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Import Modelli e Utility ---\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# --- Impostazioni ---\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clona il repository ---\n",
    "GIT_REPO_URL = \"https://github.com/Gabriele-mp/FDS-DITTO-DATI.git\"\n",
    "REPO_NAME = GIT_REPO_URL.split('/')[-1].replace('.git', '')\n",
    "\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    print(f\"Clonazione repository: {GIT_REPO_URL}...\")\n",
    "    !git clone -q {GIT_REPO_URL}\n",
    "else:\n",
    "    print(f\"Repository {REPO_NAME} giÃ  presente.\")\n",
    "\n",
    "# --- Aggiungi 'src' al path di sistema ---\n",
    "SRC_PATH = os.path.join(os.getcwd(), REPO_NAME, 'src')\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)\n",
    "    print(f\"Aggiunto '{SRC_PATH}' a sys.path\")\n",
    "\n",
    "print(\"Repository e path pronti.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELLA 3 (CORRETTA): Import dal Tuo \"Toolbox\"\n",
    "# ===================================================================\n",
    "try:\n",
    "    # --- Import per Modello 1 (il tuo) ---\n",
    "    print(\"Importazione funzioni per Modello 1 (Stacking)...\")\n",
    "    \n",
    "    # === LA RIGA MANCANTE Ãˆ QUESTA ===\n",
    "    from data_processing import load_and_clean_data\n",
    "    # ==================================\n",
    "    \n",
    "    from feature_builder import (\n",
    "        extract_features_v8,\n",
    "        extract_features_v21,\n",
    "        extract_moveset_features,\n",
    "        extract_features_CRITICAL_MISSING\n",
    "    )\n",
    "    from train_utils import build_feature_dataframe\n",
    "\n",
    "    # --- Import per Modello 2 (del tuo compagno) ---\n",
    "    print(\"Importazione funzioni per Modello 2...\")\n",
    "    from feature_builder_Model2 import (\n",
    "        extract_features_v8 as extract_v8_M2,\n",
    "        extract_features_v20 as extract_v20_M2,\n",
    "        build_feature_dataframe as build_df_M2\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Funzioni importate con successo per entrambi i modelli!\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Errore nell'import: {e}\")\n",
    "    print(\"Verifica che il path nella Cella 2 sia corretto e che i file .py in 'src/' siano aggiornati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd90adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELLA 4 (AGGIORNATA): Configurazione e Caricamento Dati\n",
    "# ===================================================================\n",
    "\n",
    "# --- Configurazione Globale ---\n",
    "COMPETITION_NAME = 'fds-pokemon-battles-prediction-2025'\n",
    "DATA_PATH = os.path.join('../input', COMPETITION_NAME)\n",
    "SEED = 123\n",
    "N_SPLITS = 5\n",
    "\n",
    "# Definisci KFold (usato da entrambi i modelli)\n",
    "kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# --- Caricamento Dati di Training ---\n",
    "print(\"Caricamento e pulizia dati di TRAINING...\")\n",
    "train_file_path = os.path.join(DATA_PATH, 'train.jsonl')\n",
    "# Usiamo la TUA funzione di pulizia (load_and_clean_data)\n",
    "df_train_shuffled = load_and_clean_data(train_file_path, seed=SEED, is_train=True)\n",
    "y_train = df_train_shuffled['player_won'].astype(int) # Target, usato da tutti\n",
    "print(f\"Dati di training pronti: {df_train_shuffled.shape}\")\n",
    "\n",
    "# --- Caricamento Dati di Test ---\n",
    "print(\"\\nCaricamento dati di TEST...\")\n",
    "test_file_path = os.path.join(DATA_PATH, 'test.jsonl')\n",
    "df_test_raw = load_and_clean_data(test_file_path, is_train=False)\n",
    "battle_ids = df_test_raw['battle_id'] # ID per la submission, usati da tutti\n",
    "print(f\"Dati di test pronti: {df_test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2607ce8",
   "metadata": {},
   "source": [
    "## ðŸ¤– Submission 1: 3-Model Stacking Ensemble\n",
    "\n",
    "This submission is generated by a **Stacking Ensemble** designed to maximize accuracy by combining the predictions of 3 heterogeneous base models.\n",
    "\n",
    "**Architecture:**\n",
    "* **Level 0 (Feature Sets):**\n",
    "    * **Set 1 (v8):** A compact set of ~30 features (used by LR).\n",
    "    * **Set 2 (v21):** A broad set of ~100+ features (used by LGBM).\n",
    "    * **Set 3 (Mega-Set):** A \"supreme\" set of 145 features (v21 + Moveset + Critical), reduced to 120 via feature selection (used by the second LGBM).\n",
    "* **Level 1 (Base Models):**\n",
    "    1.  `lr_v8`: **Optimized LogisticRegression** (on Set v8).\n",
    "    2.  `lgbm_v21`: **LightGBM** (on Set v21).\n",
    "    3.  `lgbm_mega`: **LightGBM** (on the selected Mega-Set).\n",
    "* **Level 2 (Meta-Model):**\n",
    "    * The Out-of-Fold (OOF) predictions from the 3 base models become the *meta-features* used to train a final **LogisticRegression**, which acts as a \"judge\" to weigh and combine the results.\n",
    "\n",
    "**Expected CV (Cross-Validation) Score:** 0.8528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Inizio Pipeline di Training ---\")\n",
    "\n",
    "# ===================================================================\n",
    "# 1. GENERAZIONE FEATURE SET DI TRAINING\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 1/6] Generazione Feature Set di Training...\")\n",
    "\n",
    "# Set 1 (LR)\n",
    "X_train_v8, y_train = build_feature_dataframe(df_train_shuffled, extract_features_v8, is_test_set=False)\n",
    "# Set 2 (LGBM)\n",
    "X_train_v21, _ = build_feature_dataframe(df_train_shuffled, extract_features_v21, is_test_set=False)\n",
    "# Set 3 (per Mega-Set)\n",
    "X_train_moveset, _ = build_feature_dataframe(df_train_shuffled, extract_moveset_features, is_test_set=False)\n",
    "X_train_ultimate, _ = build_feature_dataframe(df_train_shuffled, extract_features_CRITICAL_MISSING, is_test_set=False)\n",
    "\n",
    "# ===================================================================\n",
    "# 2. CREAZIONE E PULIZIA \"MEGA-SET\"\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 2/6] Creazione e Pulizia 'Mega-Set'...\")\n",
    "\n",
    "X_train_v21_safe = X_train_v21.reset_index(drop=True)\n",
    "X_train_moveset_safe = X_train_moveset.reset_index(drop=True)\n",
    "X_train_ultimate_safe = X_train_ultimate.reset_index(drop=True)\n",
    "\n",
    "X_train_MEGASUPERSET = pd.concat([X_train_v21_safe, X_train_moveset_safe, X_train_ultimate_safe], axis=1)\n",
    "X_train_MEGASUPERSET = X_train_MEGASUPERSET.loc[:,~X_train_MEGASUPERSET.columns.duplicated()]\n",
    "print(f\"Shape Mega-Set (grezzo): {X_train_MEGASUPERSET.shape}\")\n",
    "\n",
    "# Feature Selection\n",
    "lgbm_selector = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31, random_state=SEED, verbose=-1)\n",
    "lgbm_selector.fit(X_train_MEGASUPERSET, y_train)\n",
    "importances = pd.Series(lgbm_selector.feature_importances_, index=X_train_MEGASUPERSET.columns)\n",
    "\n",
    "top_120_features = importances.nlargest(120).index\n",
    "X_train_MEGA_SELECTED = X_train_MEGASUPERSET[top_120_features]\n",
    "print(f\"Shape Mega-Set (pulito): {X_train_MEGA_SELECTED.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. OTTIMIZZAZIONE LR\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 3/6] Ottimizzazione Iperparametri (LR)...\")\n",
    "\n",
    "param_grid_lr = {'model__C': [1.0, 5.0, 10.0, 15.0, 20.0, 25.0]}\n",
    "model_lr_v8_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(penalty='l2', solver='saga', max_iter=5000, random_state=SEED))\n",
    "])\n",
    "grid_lr = GridSearchCV(estimator=model_lr_v8_pipeline, param_grid=param_grid_lr, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "grid_lr.fit(X_train_v8, y_train)\n",
    "lr_v8_OPTIMIZED = grid_lr.best_estimator_\n",
    "\n",
    "print(f\"LR Ottimizzato. Miglior Score: {grid_lr.best_score_:.4f}, Migliori Parametri: {grid_lr.best_params_}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 4. DEFINIZIONE MODELLI BASE\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 4/6] Definizione Modelli Base L1...\")\n",
    "base_models = {}\n",
    "\n",
    "# Modello 1 (LR Ottimizzato)\n",
    "base_models['lr_v8'] = (lr_v8_OPTIMIZED, X_train_v8)\n",
    "\n",
    "# Modello 2 (LGBM v21)\n",
    "lgbm_v21 = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31, random_state=SEED, verbose=-1)\n",
    "base_models['lgbm_v21'] = (lgbm_v21, X_train_v21)\n",
    "\n",
    "# Modello 3 (LGBM Mega)\n",
    "lgbm_mega = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31, random_state=SEED, verbose=-1)\n",
    "base_models['lgbm_mega'] = (lgbm_mega, X_train_MEGA_SELECTED)\n",
    "\n",
    "print(f\"Definiti {len(base_models)} modelli base: {list(base_models.keys())}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 5. ESECUZIONE STACKING (OOF)\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 5/6] Esecuzione Stacking (OOF)...\")\n",
    "\n",
    "X_meta_df = pd.DataFrame(np.zeros((len(y_train), len(base_models))), columns=base_models.keys())\n",
    "final_base_models = {} # Modelli addestrati su tutto il training set\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(y_train, y_train), total=N_SPLITS, desc=\"Folds\")):\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    for name, (model, X_data) in base_models.items():\n",
    "        X_train_fold = X_data.iloc[train_idx]\n",
    "        X_val_fold = X_data.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        X_meta_df.loc[val_idx, name] = model.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "print(\"Addestramento modelli base finali su tutti i dati...\")\n",
    "for name, (model, X_data) in tqdm(base_models.items(), desc=\"Modelli Finali L1\"):\n",
    "    final_base_models[name] = model.fit(X_data, y_train)\n",
    "\n",
    "print(\"âœ… Meta-Features (X_meta_df) create.\")\n",
    "\n",
    "# ===================================================================\n",
    "# 6. ADDESTRAMENTO META-MODELLO L2\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 6/6] Addestramento Meta-Modello L2...\")\n",
    "\n",
    "# Analisi e selezione (usiamo tutti e 3 i modelli come da analisi precedente)\n",
    "optimal_models = ['lr_v8', 'lgbm_v21', 'lgbm_mega']\n",
    "X_meta_df_optimal = X_meta_df[optimal_models].copy()\n",
    "\n",
    "# Addestra Meta-Modello Finale\n",
    "meta_model = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "final_ensemble_model = meta_model.fit(X_meta_df_optimal, y_train)\n",
    "\n",
    "# Stampa CV score\n",
    "final_cv_score = cross_val_score(meta_model, X_meta_df_optimal, y_train, cv=kfold, scoring='accuracy', n_jobs=-1).mean()\n",
    "\n",
    "print(f\"âœ… Pipeline di Training completata.\")\n",
    "print(f\"ðŸŽ¯ CV Score Finale (stimato): {final_cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cfe24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Inizio Pipeline di Predizione (Test Set) ---\")\n",
    "\n",
    "# ===================================================================\n",
    "# 1. CARICAMENTO DATI DI TEST\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 1/3] Caricamento Dati di Test...\")\n",
    "test_file_path = os.path.join(DATA_PATH, 'test.jsonl')\n",
    "df_test_raw = load_and_clean_data(test_file_path, is_train=False)\n",
    "battle_ids = df_test_raw['battle_id']\n",
    "\n",
    "# ===================================================================\n",
    "# 2. GENERAZIONE FEATURE SET DI TEST\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 2/3] Generazione Feature Set di Test...\")\n",
    "\n",
    "# Set 1 (LR)\n",
    "X_test_v8, _ = build_feature_dataframe(df_test_raw, extract_features_v8, is_test_set=True)\n",
    "X_test_v8 = X_test_v8[X_train_v8.columns] # Allinea colonne\n",
    "\n",
    "# Set 2 (LGBM)\n",
    "X_test_v21, _ = build_feature_dataframe(df_test_raw, extract_features_v21, is_test_set=True)\n",
    "X_test_v21 = X_test_v21[X_train_v21.columns] # Allinea colonne\n",
    "\n",
    "# Set 3 (per Mega-Set)\n",
    "X_test_moveset, _ = build_feature_dataframe(df_test_raw, extract_moveset_features, is_test_set=True)\n",
    "X_test_moveset = X_test_moveset[X_train_moveset.columns] # Allinea colonne\n",
    "X_test_ultimate, _ = build_feature_dataframe(df_test_raw, extract_features_CRITICAL_MISSING, is_test_set=True)\n",
    "X_test_ultimate = X_test_ultimate[X_train_ultimate.columns] # Allinea colonne\n",
    "\n",
    "# Costruisci X_test_MEGA_SELECTED\n",
    "X_test_v21_safe = X_test_v21.reset_index(drop=True)\n",
    "X_test_moveset_safe = X_test_moveset.reset_index(drop=True)\n",
    "X_test_ultimate_safe = X_test_ultimate.reset_index(drop=True)\n",
    "\n",
    "X_test_MEGASUPERSET = pd.concat([X_test_v21_safe, X_test_moveset_safe, X_test_ultimate_safe], axis=1)\n",
    "X_test_MEGASUPERSET = X_test_MEGASUPERSET.loc[:,~X_test_MEGASUPERSET.columns.duplicated()]\n",
    "\n",
    "# Usa le top_120_features definite nella Cella 6\n",
    "X_test_MEGA_SELECTED = X_test_MEGASUPERSET[top_120_features]\n",
    "print(f\"Shape X_test_MEGA_SELECTED: {X_test_MEGA_SELECTED.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. GENERAZIONE SUBMISSION\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 3/3] Generazione Submission Finale...\")\n",
    "\n",
    "# Mappa per i dati di test\n",
    "test_data_map = {\n",
    "    'lr_v8': X_test_v8,\n",
    "    'lgbm_v21': X_test_v21,\n",
    "    'lgbm_mega': X_test_MEGA_SELECTED\n",
    "}\n",
    "\n",
    "# Crea Meta-Features di Test L2\n",
    "X_meta_test_df = pd.DataFrame(columns=optimal_models)\n",
    "for name, model in tqdm(final_base_models.items(), desc=\"Predizioni Test L1\"):\n",
    "    if name in optimal_models:\n",
    "        X_test_data = test_data_map.get(name)\n",
    "        X_meta_test_df[name] = model.predict_proba(X_test_data)[:, 1]\n",
    "\n",
    "# Predizione Finale L2\n",
    "final_predictions = final_ensemble_model.predict(X_meta_test_df[optimal_models])\n",
    "\n",
    "# --- Creazione File Submission ---\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': battle_ids,\n",
    "    'player_won': final_predictions.astype(int)\n",
    "})\n",
    "\n",
    "submission_filename = 'submission.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nâœ… File '{submission_filename}' creato con successo!\")\n",
    "print(f\"ðŸ“Š Predizioni: {len(final_predictions)}\")\n",
    "print(f\"ðŸ“ˆ Distribuzione (0 vs 1): {np.bincount(final_predictions)}\")\n",
    "print(f\"ðŸŽ¯ CV Score Atteso: {final_cv_score:.4f}\")\n",
    "\n",
    "display(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a26bf",
   "metadata": {},
   "source": [
    "## ðŸ¤– Submission 2: 2-Model Stacking Ensemble (LR + XGB)\n",
    "\n",
    "This submission is generated by a 2-model stacking ensemble, based on the `Model2` development pipeline.\n",
    "\n",
    "**Architecture:**\n",
    "* **Level 0 (Feature Sets):**\n",
    "    * **Set 1 (v8_M2):** The compact `v8` feature set (from `feature_builder_Model2.py`) used by the Logistic Regression.\n",
    "    * **Set 2 (v20_M2):** The larger `v20` feature set (from `feature_builder_Model2.py`) used by the XGBoost model.\n",
    "* **Level 1 (Base Models):**\n",
    "    1.  `lr_v8`: **LogisticRegression** (on Set v8_M2).\n",
    "    2.  `xgb_v20`: **XGBoost** (on Set v20_M2).\n",
    "* **Level 2 (Meta-Model):**\n",
    "    * The OOF predictions from these two models are used as meta-features to train a final **LogisticRegression** meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELLA 9 (CORRETTA): PIPELINE MODELLO 2 (dal notebook del compagno)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Inizio Pipeline Modello 2: Stacking (LR+XGB) ---\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Generazione Feature (da 02_Feature_Engineering) ---\n",
    "    print(\"[M2 - Fase 1/5] Generazione Feature Set (v8, v20)...\")\n",
    "    \n",
    "    # Usiamo le funzioni _M2 importate con alias\n",
    "    # Il train set restituisce 2 valori (X, y) - QUESTO Ãˆ CORRETTO\n",
    "    X_train_v8_M2, _ = build_df_M2(df_train_shuffled, extract_v8_M2, is_test_set=False)\n",
    "    X_train_v20_M2, _ = build_df_M2(df_train_shuffled, extract_v20_M2, is_test_set=False)\n",
    "    \n",
    "    # === CORREZIONE QUI ===\n",
    "    # Il test set restituisce 1 valore (X) - Rimuoviamo il \", _\"\n",
    "    print(\"\\nGenerazione feature di test per M2...\")\n",
    "    X_test_v8_M2 = build_df_M2(df_test_raw, extract_v8_M2, is_test_set=True)\n",
    "    X_test_v20_M2 = build_df_M2(df_test_raw, extract_v20_M2, is_test_set=True)\n",
    "    # === FINE CORREZIONE ===\n",
    "    \n",
    "    # Allinea colonne\n",
    "    X_test_v8_M2 = X_test_v8_M2[X_train_v8_M2.columns] \n",
    "    X_test_v20_M2 = X_test_v20_M2[X_train_v20_M2.columns]\n",
    "    \n",
    "    print(f\"Feature M2 (Train): v8({X_train_v8_M2.shape}), v20({X_train_v20_M2.shape})\")\n",
    "    print(f\"Feature M2 (Test): v8({X_test_v8_M2.shape}), v20({X_test_v20_M2.shape})\")\n",
    "\n",
    "    # --- 2. Definizione Modelli Base (L0) (da 02_All_Base_Models_Training) ---\n",
    "    print(\"\\n[M2 - Fase 2/5] Definizione Modelli Base (L0)...\")\n",
    "    base_models_M2 = {}\n",
    "\n",
    "    # Modello LR (v8)\n",
    "    model_lr_v8_M2 = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(C=10.0, penalty='l2', solver='saga', max_iter=5000, random_state=SEED))\n",
    "    ])\n",
    "    base_models_M2['lr_v8'] = (model_lr_v8_M2, X_train_v8_M2)\n",
    "\n",
    "    # Modello XGB (v20)\n",
    "    model_xgb_v20_M2 = XGBClassifier(\n",
    "        colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200,\n",
    "        reg_lambda=5, subsample=0.7, objective='binary:logistic',\n",
    "        eval_metric='logloss', use_label_encoder=False, random_state=SEED\n",
    "    )\n",
    "    base_models_M2['xgb_v20'] = (model_xgb_v20_M2, X_train_v20_M2)\n",
    "    \n",
    "    optimal_models_M2 = ['lr_v8', 'xgb_v20']\n",
    "\n",
    "    # --- 3. Stacking (OOF) (da 02_All_Base_Models_Training) ---\n",
    "    print(\"\\n[M2 - Fase 3/5] Esecuzione Stacking (OOF)...\")\n",
    "    \n",
    "    # y_train Ã¨ giÃ  definito nella Cella 4\n",
    "    X_meta_train_M2 = pd.DataFrame(np.zeros((len(y_train), len(optimal_models_M2))), columns=optimal_models_M2)\n",
    "    final_base_models_M2 = {} \n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(y_train, y_train), total=N_SPLITS, desc=\"[M2] Folds\")):\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        for name, (model, X_data) in base_models_M2.items():\n",
    "            if name in optimal_models_M2:\n",
    "                X_train_fold = X_data.iloc[train_idx]\n",
    "                X_val_fold = X_data.iloc[val_idx]\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                X_meta_train_M2.loc[val_idx, name] = model.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "    print(\"Addestramento modelli base L0 finali (su tutti i dati)...\")\n",
    "    for name, (model, X_data) in tqdm(base_models_M2.items(), desc=\"[M2] Modelli Finali L0\"):\n",
    "        if name in optimal_models_M2:\n",
    "            final_base_models_M2[name] = model.fit(X_data, y_train)\n",
    "\n",
    "    # --- 4. Addestramento Meta-Modello (L1) (da 02_Ensemble_Submission) ---\n",
    "    print(\"\\n[M2 - Fase 4/5] Addestramento Meta-Modello (L1)...\")\n",
    "    \n",
    "    final_ensemble_model_M2 = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "    final_ensemble_model_M2.fit(X_meta_train_M2, y_train)\n",
    "    print(\"Meta-Modello (L1) addestrato.\")\n",
    "\n",
    "    # --- 5. Predizione e Salvataggio (da 02_Ensemble_Submission) ---\n",
    "    print(\"\\n[M2 - Fase 5/5] Generazione Submission Modello 2...\")\n",
    "    \n",
    "    X_meta_test_M2 = pd.DataFrame()\n",
    "    X_meta_test_M2['lr_v8'] = final_base_models_M2['lr_v8'].predict_proba(X_test_v8_M2)[:, 1]\n",
    "    X_meta_test_M2['xgb_v20'] = final_base_models_M2['xgb_v20'].predict_proba(X_test_v20_M2)[:, 1]\n",
    "\n",
    "    final_predictions_M2 = final_ensemble_model_M2.predict(X_meta_test_M2)\n",
    "\n",
    "    submission_df_2 = pd.DataFrame({\n",
    "        'battle_id': battle_ids,\n",
    "        'player_won': final_predictions_M2.astype(int)\n",
    "    })\n",
    "\n",
    "    submission_filename_2 = 'submission_model_2.csv'\n",
    "    submission_df_2.to_csv(submission_filename_2, index=False)\n",
    "\n",
    "    print(f\"âœ… File '{submission_filename_2}' creato con successo!\")\n",
    "    display(submission_df_2.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!!! ERRORE DURANTE LA PIPELINE DEL MODELLO 2 !!!\")\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

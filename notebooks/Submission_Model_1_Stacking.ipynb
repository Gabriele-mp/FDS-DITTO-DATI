{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Installazioni necessarie (per Kaggle) ---\n",
    "# !pip install -q xgboost catboost lightgbm\n",
    "\n",
    "# --- Import di base ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Import Modelli e Utility ---\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# --- Impostazioni ---\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clona il repository ---\n",
    "GIT_REPO_URL = \"https://github.com/Gabriele-mp/FDS-DITTO-DATI.git\"\n",
    "REPO_NAME = GIT_REPO_URL.split('/')[-1].replace('.git', '')\n",
    "\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    print(f\"Clonazione repository: {GIT_REPO_URL}...\")\n",
    "    !git clone -q {GIT_REPO_URL}\n",
    "else:\n",
    "    print(f\"Repository {REPO_NAME} giÃ  presente.\")\n",
    "\n",
    "# --- Aggiungi 'src' al path di sistema ---\n",
    "SRC_PATH = os.path.join(os.getcwd(), REPO_NAME, 'src')\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)\n",
    "    print(f\"Aggiunto '{SRC_PATH}' a sys.path\")\n",
    "\n",
    "print(\"Repository e path pronti.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import dal tuo repository GitHub ---\n",
    "try:\n",
    "    # Funzioni per caricare e pulire\n",
    "    from data_processing import load_and_clean_data\n",
    "    \n",
    "    # Funzione per costruire i DataFrame\n",
    "    from train_utils import build_feature_dataframe\n",
    "    \n",
    "    # TUTTE le funzioni di feature engineering\n",
    "    from feature_builder import (\n",
    "        extract_features_v8,\n",
    "        extract_features_v21,\n",
    "        extract_moveset_features,\n",
    "        extract_features_CRITICAL_MISSING\n",
    "    )\n",
    "    print(\"âœ… Funzioni importate con successo dal repository GitHub!\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Errore nell'import: {e}\")\n",
    "    print(\"Verifica che il path nella Cella 2 sia corretto e che src/__init__.py esista.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd90adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurazione Globale ---\n",
    "COMPETITION_NAME = 'fds-pokemon-battles-prediction-2025'\n",
    "DATA_PATH = os.path.join('../input', COMPETITION_NAME)\n",
    "SEED = 123\n",
    "N_SPLITS = 5\n",
    "\n",
    "# --- Caricamento e Pulizia Dati di Training ---\n",
    "train_file_path = os.path.join(DATA_PATH, 'train.jsonl')\n",
    "df_train_shuffled = load_and_clean_data(train_file_path, seed=SEED, is_train=True)\n",
    "\n",
    "# Definisci KFold \n",
    "kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "print(f\"\\nDati di training pronti: {df_train_shuffled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd478937",
   "metadata": {},
   "source": [
    "## ðŸ¤– Spiegazione Modello 1: Stacking Ensemble a 3 Livelli\n",
    "\n",
    "Questa submission Ã¨ generata da un **Ensemble di Stacking** progettato per massimizzare l'accuratezza combinando le previsioni di 3 modelli base eterogenei.\n",
    "\n",
    "**Architettura:**\n",
    "* **Livello 0 (Feature Sets):**\n",
    "    * **Set 1 (v8):** Un set compatto di ~30 feature (usato da LR).\n",
    "    * **Set 2 (v21):** Un set ampio di ~100+ feature (usato da LGBM).\n",
    "    * **Set 3 (Mega-Set):** Un set \"supremo\" di 145 feature (v21 + Moveset + Critical), ridotto a 120 tramite feature selection (usato dal secondo LGBM).\n",
    "* **Livello 1 (Modelli Base):**\n",
    "    1.  `lr_v8`: **LogisticRegression** ottimizzata (su Set v8).\n",
    "    2.  `lgbm_v21`: **LightGBM** (su Set v21).\n",
    "    3.  `lgbm_mega`: **LightGBM** (sul Mega-Set pulito).\n",
    "* **Livello 2 (Meta-Modello):**\n",
    "    * Le previsioni Out-of-Fold (OOF) dei 3 modelli base diventano le *meta-features* che addestrano una **LogisticRegression** finale, che funge da \"giudice\" per pesare e combinare i risultati.\n",
    "\n",
    "**CV Attesa:** 0.8528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Inizio Pipeline di Training ---\")\n",
    "\n",
    "# ===================================================================\n",
    "# 1. GENERAZIONE FEATURE SET DI TRAINING\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 1/6] Generazione Feature Set di Training...\")\n",
    "\n",
    "# Set 1 (LR)\n",
    "X_train_v8, y_train = build_feature_dataframe(df_train_shuffled, extract_features_v8, is_test_set=False)\n",
    "# Set 2 (LGBM)\n",
    "X_train_v21, _ = build_feature_dataframe(df_train_shuffled, extract_features_v21, is_test_set=False)\n",
    "# Set 3 (per Mega-Set)\n",
    "X_train_moveset, _ = build_feature_dataframe(df_train_shuffled, extract_moveset_features, is_test_set=False)\n",
    "X_train_ultimate, _ = build_feature_dataframe(df_train_shuffled, extract_features_CRITICAL_MISSING, is_test_set=False)\n",
    "\n",
    "# ===================================================================\n",
    "# 2. CREAZIONE E PULIZIA \"MEGA-SET\"\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 2/6] Creazione e Pulizia 'Mega-Set'...\")\n",
    "\n",
    "X_train_v21_safe = X_train_v21.reset_index(drop=True)\n",
    "X_train_moveset_safe = X_train_moveset.reset_index(drop=True)\n",
    "X_train_ultimate_safe = X_train_ultimate.reset_index(drop=True)\n",
    "\n",
    "X_train_MEGASUPERSET = pd.concat([X_train_v21_safe, X_train_moveset_safe, X_train_ultimate_safe], axis=1)\n",
    "X_train_MEGASUPERSET = X_train_MEGASUPERSET.loc[:,~X_train_MEGASUPERSET.columns.duplicated()]\n",
    "print(f\"Shape Mega-Set (grezzo): {X_train_MEGASUPERSET.shape}\")\n",
    "\n",
    "# Feature Selection\n",
    "lgbm_selector = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31, random_state=SEED, verbose=-1)\n",
    "lgbm_selector.fit(X_train_MEGASUPERSET, y_train)\n",
    "importances = pd.Series(lgbm_selector.feature_importances_, index=X_train_MEGASUPERSET.columns)\n",
    "\n",
    "top_120_features = importances.nlargest(120).index\n",
    "X_train_MEGA_SELECTED = X_train_MEGASUPERSET[top_120_features]\n",
    "print(f\"Shape Mega-Set (pulito): {X_train_MEGA_SELECTED.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. OTTIMIZZAZIONE LR\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 3/6] Ottimizzazione Iperparametri (LR)...\")\n",
    "\n",
    "param_grid_lr = {'model__C': [1.0, 5.0, 10.0, 15.0, 20.0, 25.0]}\n",
    "model_lr_v8_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(penalty='l2', solver='saga', max_iter=5000, random_state=SEED))\n",
    "])\n",
    "grid_lr = GridSearchCV(estimator=model_lr_v8_pipeline, param_grid=param_grid_lr, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "grid_lr.fit(X_train_v8, y_train)\n",
    "lr_v8_OPTIMIZED = grid_lr.best_estimator_\n",
    "\n",
    "print(f\"LR Ottimizzato. Miglior Score: {grid_lr.best_score_:.4f}, Migliori Parametri: {grid_lr.best_params_}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 4. DEFINIZIONE MODELLI BASE\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 4/6] Definizione Modelli Base L1...\")\n",
    "base_models = {}\n",
    "\n",
    "# Modello 1 (LR Ottimizzato)\n",
    "base_models['lr_v8'] = (lr_v8_OPTIMIZED, X_train_v8)\n",
    "\n",
    "# Modello 2 (LGBM v21)\n",
    "lgbm_v21 = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31, random_state=SEED, verbose=-1)\n",
    "base_models['lgbm_v21'] = (lgbm_v21, X_train_v21)\n",
    "\n",
    "# Modello 3 (LGBM Mega)\n",
    "lgbm_mega = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31, random_state=SEED, verbose=-1)\n",
    "base_models['lgbm_mega'] = (lgbm_mega, X_train_MEGA_SELECTED)\n",
    "\n",
    "print(f\"Definiti {len(base_models)} modelli base: {list(base_models.keys())}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 5. ESECUZIONE STACKING (OOF)\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 5/6] Esecuzione Stacking (OOF)...\")\n",
    "\n",
    "X_meta_df = pd.DataFrame(np.zeros((len(y_train), len(base_models))), columns=base_models.keys())\n",
    "final_base_models = {} # Modelli addestrati su tutto il training set\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(y_train, y_train), total=N_SPLITS, desc=\"Folds\")):\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    for name, (model, X_data) in base_models.items():\n",
    "        X_train_fold = X_data.iloc[train_idx]\n",
    "        X_val_fold = X_data.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        X_meta_df.loc[val_idx, name] = model.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "print(\"Addestramento modelli base finali su tutti i dati...\")\n",
    "for name, (model, X_data) in tqdm(base_models.items(), desc=\"Modelli Finali L1\"):\n",
    "    final_base_models[name] = model.fit(X_data, y_train)\n",
    "\n",
    "print(\"âœ… Meta-Features (X_meta_df) create.\")\n",
    "\n",
    "# ===================================================================\n",
    "# 6. ADDESTRAMENTO META-MODELLO L2\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 6/6] Addestramento Meta-Modello L2...\")\n",
    "\n",
    "# Analisi e selezione (usiamo tutti e 3 i modelli come da analisi precedente)\n",
    "optimal_models = ['lr_v8', 'lgbm_v21', 'lgbm_mega']\n",
    "X_meta_df_optimal = X_meta_df[optimal_models].copy()\n",
    "\n",
    "# Addestra Meta-Modello Finale\n",
    "meta_model = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "final_ensemble_model = meta_model.fit(X_meta_df_optimal, y_train)\n",
    "\n",
    "# Stampa CV score\n",
    "final_cv_score = cross_val_score(meta_model, X_meta_df_optimal, y_train, cv=kfold, scoring='accuracy', n_jobs=-1).mean()\n",
    "\n",
    "print(f\"âœ… Pipeline di Training completata.\")\n",
    "print(f\"ðŸŽ¯ CV Score Finale (stimato): {final_cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cfe24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Inizio Pipeline di Predizione (Test Set) ---\")\n",
    "\n",
    "# ===================================================================\n",
    "# 1. CARICAMENTO DATI DI TEST\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 1/3] Caricamento Dati di Test...\")\n",
    "test_file_path = os.path.join(DATA_PATH, 'test.jsonl')\n",
    "df_test_raw = load_and_clean_data(test_file_path, is_train=False)\n",
    "battle_ids = df_test_raw['battle_id']\n",
    "\n",
    "# ===================================================================\n",
    "# 2. GENERAZIONE FEATURE SET DI TEST\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 2/3] Generazione Feature Set di Test...\")\n",
    "\n",
    "# Set 1 (LR)\n",
    "X_test_v8, _ = build_feature_dataframe(df_test_raw, extract_features_v8, is_test_set=True)\n",
    "X_test_v8 = X_test_v8[X_train_v8.columns] # Allinea colonne\n",
    "\n",
    "# Set 2 (LGBM)\n",
    "X_test_v21, _ = build_feature_dataframe(df_test_raw, extract_features_v21, is_test_set=True)\n",
    "X_test_v21 = X_test_v21[X_train_v21.columns] # Allinea colonne\n",
    "\n",
    "# Set 3 (per Mega-Set)\n",
    "X_test_moveset, _ = build_feature_dataframe(df_test_raw, extract_moveset_features, is_test_set=True)\n",
    "X_test_moveset = X_test_moveset[X_train_moveset.columns] # Allinea colonne\n",
    "X_test_ultimate, _ = build_feature_dataframe(df_test_raw, extract_features_CRITICAL_MISSING, is_test_set=True)\n",
    "X_test_ultimate = X_test_ultimate[X_train_ultimate.columns] # Allinea colonne\n",
    "\n",
    "# Costruisci X_test_MEGA_SELECTED\n",
    "X_test_v21_safe = X_test_v21.reset_index(drop=True)\n",
    "X_test_moveset_safe = X_test_moveset.reset_index(drop=True)\n",
    "X_test_ultimate_safe = X_test_ultimate.reset_index(drop=True)\n",
    "\n",
    "X_test_MEGASUPERSET = pd.concat([X_test_v21_safe, X_test_moveset_safe, X_test_ultimate_safe], axis=1)\n",
    "X_test_MEGASUPERSET = X_test_MEGASUPERSET.loc[:,~X_test_MEGASUPERSET.columns.duplicated()]\n",
    "\n",
    "# Usa le top_120_features definite nella Cella 6\n",
    "X_test_MEGA_SELECTED = X_test_MEGASUPERSET[top_120_features]\n",
    "print(f\"Shape X_test_MEGA_SELECTED: {X_test_MEGA_SELECTED.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. GENERAZIONE SUBMISSION\n",
    "# ===================================================================\n",
    "print(\"\\n[Fase 3/3] Generazione Submission Finale...\")\n",
    "\n",
    "# Mappa per i dati di test\n",
    "test_data_map = {\n",
    "    'lr_v8': X_test_v8,\n",
    "    'lgbm_v21': X_test_v21,\n",
    "    'lgbm_mega': X_test_MEGA_SELECTED\n",
    "}\n",
    "\n",
    "# Crea Meta-Features di Test L2\n",
    "X_meta_test_df = pd.DataFrame(columns=optimal_models)\n",
    "for name, model in tqdm(final_base_models.items(), desc=\"Predizioni Test L1\"):\n",
    "    if name in optimal_models:\n",
    "        X_test_data = test_data_map.get(name)\n",
    "        X_meta_test_df[name] = model.predict_proba(X_test_data)[:, 1]\n",
    "\n",
    "# Predizione Finale L2\n",
    "final_predictions = final_ensemble_model.predict(X_meta_test_df[optimal_models])\n",
    "\n",
    "# --- Creazione File Submission ---\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': battle_ids,\n",
    "    'player_won': final_predictions.astype(int)\n",
    "})\n",
    "\n",
    "submission_filename = 'submission.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"\\nâœ… File '{submission_filename}' creato con successo!\")\n",
    "print(f\"ðŸ“Š Predizioni: {len(final_predictions)}\")\n",
    "print(f\"ðŸ“ˆ Distribuzione (0 vs 1): {np.bincount(final_predictions)}\")\n",
    "print(f\"ðŸŽ¯ CV Score Atteso: {final_cv_score:.4f}\")\n",
    "\n",
    "display(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

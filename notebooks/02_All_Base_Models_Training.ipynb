{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b2c73c3",
   "metadata": {},
   "source": [
    "Importing libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e876f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib \n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === SEED  ===\n",
    "SEED = 123 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f4261",
   "metadata": {},
   "source": [
    "Loading Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading feature sets from 'data/processed/'...\")\n",
    "\n",
    "try:\n",
    "    X_train_v8 = pd.read_csv('../data/processed/v8_train_features.csv')\n",
    "    X_train_v20 = pd.read_csv('../data/processed/v20_train_features.csv')\n",
    "    X_train_v19 = pd.read_csv('../data/processed/v19_train_features.csv')\n",
    "    X_train_v2 = pd.read_csv('../data/processed/v2_train_features.csv')\n",
    "\n",
    "    y_train = pd.read_csv('../data/processed/train_target.csv').squeeze() \n",
    "\n",
    "    print(\"Dati caricati con successo.\")\n",
    "    print(f\"Shape y_train: {y_train.shape}\")\n",
    "    print(f\"Shape X_train_v8 (for LR): {X_train_v8.shape}\")\n",
    "    print(f\"Shape X_train_v20 (for XGB): {X_train_v20.shape}\")\n",
    "    print(f\"Shape X_train_v19 (for RF/CAT/kNN): {X_train_v19.shape}\")\n",
    "    print(f\"Shape X_train_v2 (for XGB-v2): {X_train_v2.shape}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: File not found. {e}\")\n",
    "    print(\"!!! Be sure to execute notebook 02_Feature_Engineering.ipynb first!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8b34d",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e3712",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = {}\n",
    "\n",
    "# === 1. Model LR (v8) ===\n",
    "model_lr_v8 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(\n",
    "        C=10.0, \n",
    "        penalty='l2', \n",
    "        solver='saga', \n",
    "        max_iter=5000, \n",
    "        random_state=SEED\n",
    "    ))\n",
    "])\n",
    "base_models['lr_v8'] = (model_lr_v8, X_train_v8)\n",
    "\n",
    "# === 2. Model XGB (v20) ===\n",
    "model_xgb_v20 = XGBClassifier(\n",
    "    colsample_bytree=0.7,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    n_estimators=200,\n",
    "    reg_lambda=5,\n",
    "    subsample=0.7,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=SEED\n",
    ")\n",
    "base_models['xgb_v20'] = (model_xgb_v20, X_train_v20)\n",
    "\n",
    "# === 3. Model RF (v19) ===\n",
    "model_rf_v19 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=0.5,\n",
    "        max_depth=10,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "base_models['rf_v19'] = (model_rf_v19, X_train_v19)\n",
    "\n",
    "# === 4. Model CAT (v19) ===\n",
    "model_cat_v19 = CatBoostClassifier(\n",
    "    learning_rate=0.03,\n",
    "    l2_leaf_reg=7,\n",
    "    iterations=300,\n",
    "    depth=8,\n",
    "    random_state=SEED,\n",
    "    verbose=0,\n",
    "    eval_metric='Accuracy'\n",
    ")\n",
    "base_models['cat_v19'] = (model_cat_v19, X_train_v19)\n",
    "\n",
    "# === 5. Model kNN (v19) ===\n",
    "model_knn_v19 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', KNeighborsClassifier(\n",
    "        metric='manhattan',\n",
    "        n_neighbors=45,\n",
    "        weights='uniform',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "base_models['knn_v19'] = (model_knn_v19, X_train_v19)\n",
    "\n",
    "\n",
    "print(f\"Definiti {len(base_models)} modelli base pronti per lo stacking.\")\n",
    "print(f\"Modelli nello stack: {list(base_models.keys())}\")\n",
    "\n",
    "# === 6. Model XGB (v2_features) ===\n",
    "model_xgb_v2 = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=SEED,\n",
    "    n_estimators=1133,  \n",
    "    alpha=1.3799959101127168,\n",
    "    colsample_bytree=0.788820517112247,\n",
    "    reg_lambda=0.8263346953150125, \n",
    "    learning_rate=0.013127281348238786,\n",
    "    max_depth=3,\n",
    "    subsample=0.7016566351370807,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "base_models['xgb_v2'] = (model_xgb_v2, X_train_v2)\n",
    "\n",
    "print(f\"\\nADDED Modello 'xgb_v2'. Total base models: {len(base_models)}\")\n",
    "print(f\"Models: {list(base_models.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Creating dataframe x_meta\n",
    "oof_preds = np.zeros((len(y_train), len(base_models)))\n",
    "X_meta_df = pd.DataFrame(oof_preds, columns=base_models.keys())\n",
    "\n",
    "final_base_models = {}\n",
    "\n",
    "print(f\"Avvio Stacking (OOF) con {N_SPLITS} folds...\")\n",
    "\n",
    "# Let'use a tqdm bar\n",
    "for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(y_train, y_train), total=N_SPLITS, desc=\"Folds\")):\n",
    "    \n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    for name, (model, X_data) in base_models.items():\n",
    "        \n",
    "        X_train_fold = X_data.iloc[train_idx]\n",
    "        X_val_fold = X_data.iloc[val_idx]\n",
    "        \n",
    "        # Training the model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Saving OOF predictions (probabilities)\n",
    "        X_meta_df.loc[val_idx, name] = model.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "print(\"\\nCreation meta features (X_meta_df) completed.\")\n",
    "\n",
    "# Now, train base models on the entire training set.\n",
    "\n",
    "print(\"Addestramento modelli base finali su tutti i dati di training...\")\n",
    "for name, (model, X_data) in tqdm(base_models.items(), desc=\"Modelli Finali\"):\n",
    "    final_base_models[name] = model.fit(X_data, y_train)\n",
    "\n",
    "print(\"Modelli base finali addestrati.\")\n",
    "display(X_meta_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0244859",
   "metadata": {},
   "source": [
    "Ensemble Analysis - Selection optimal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANALISI ENSEMBLE v6 - SELEZIONE MODELLI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 1: INDIVIDUAL PERFORMACE (OOF)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METODO 1: PERFORMANCE INDIVIDUALI DEI MODELLI BASE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "individual_scores = {}\n",
    "for name in X_meta_df.columns:\n",
    "    # Predict with threshold 0.5\n",
    "    predictions = (X_meta_df[name] > 0.5).astype(int)\n",
    "    accuracy = (predictions == y_train).mean()\n",
    "    individual_scores[name] = accuracy\n",
    "    \n",
    "\n",
    "sorted_scores = dict(sorted(individual_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"\\nINDIVIDUAL OOF ACCURACY:\")\n",
    "print(\"-\" * 50)\n",
    "for name, score in sorted_scores.items():\n",
    "    print(f\"{name:10s}: {score:.4f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "keys = list(sorted_scores.keys())\n",
    "values = list(sorted_scores.values())\n",
    "plt.barh(keys, values, color='steelblue')\n",
    "plt.xlabel('OOF Accuracy')\n",
    "plt.title('Individual performance base models (v6)')\n",
    "plt.xlim([min(values)-0.01, max(values)+0.01])\n",
    "for i, (name, score) in enumerate(sorted_scores.items()):\n",
    "    plt.text(score, i, f' {score:.4f}', va='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"v6_individual_performance.png\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# METODO 2: CORRELAZIONE PREDIZIONI\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 2: PREDICTION CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "correlation_matrix = X_meta_df.corr()\n",
    "print(\"\\nCORRELATION MATRIX:\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    fmt=\".3f\"\n",
    ")\n",
    "plt.title(f\"Base Models Correlation (Stack {list(X_meta_df.columns)})\")\n",
    "plt.savefig(\"v6_correlation_heatmap.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  COPPIE AD ALTA CORRELAZIONE (>0.90 - Potenziale Ridondanza):\")\n",
    "print(\"-\" * 50)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if corr_value > 0.90:\n",
    "            pair = (correlation_matrix.columns[i], correlation_matrix.columns[j], corr_value)\n",
    "            high_corr_pairs.append(pair)\n",
    "            print(f\"{pair[0]:10s} <-> {pair[1]:10s}: {pair[2]:.4f}\")\n",
    "\n",
    "if not high_corr_pairs:\n",
    "    print(\"‚úì Nessuna coppia con correlazione >0.90 (Buona diversit√†!)\")\n",
    "\n",
    "# Avarage correlation\n",
    "avg_corr = {}\n",
    "for col in correlation_matrix.columns:\n",
    "    \n",
    "    other_corrs = correlation_matrix[col].drop(col)\n",
    "    avg_corr[col] = other_corrs.mean()\n",
    "\n",
    "print(\"\\nüìä MEDIA CORRELAZIONE CON ALTRI MODELLI:\")\n",
    "print(\"-\" * 50)\n",
    "for name, avg in sorted(avg_corr.items(), key=lambda x: x[1]):\n",
    "    print(f\"{name:10s}: {avg:.4f} {'‚≠ê (Pi√π diverso)' if avg == min(avg_corr.values()) else ''}\")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 3: BACKWARD ELIMINATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METODO 3: BACKWARD ELIMINATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "if 'meta_model' not in locals():\n",
    "    meta_model = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "if 'kfold' not in locals():\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "# Score baseline\n",
    "baseline_score = cross_val_score(\n",
    "    meta_model, X_meta_df, y_train, cv=kfold, scoring='accuracy', n_jobs=-1\n",
    ").mean()\n",
    "\n",
    "print(f\"üìå BASELINE (Tutti i {len(X_meta_df.columns)} modelli): {baseline_score:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Remove each model\n",
    "removal_impact = {}\n",
    "for col_to_remove in X_meta_df.columns:\n",
    "    X_reduced = X_meta_df.drop(columns=[col_to_remove])\n",
    "    score = cross_val_score(\n",
    "        meta_model, X_reduced, y_train, cv=kfold, scoring='accuracy', n_jobs=-1\n",
    "    ).mean()\n",
    "    impact = score - baseline_score\n",
    "    removal_impact[col_to_remove] = {'score': score, 'impact': impact}\n",
    "    emoji = \"üìâ\" if impact < 0 else \"üìà\" if impact > 0 else \"‚û°Ô∏è\"\n",
    "    print(f\"{emoji} Senza {col_to_remove:10s}: {score:.4f} (Œî = {impact:+.4f})\")\n",
    "\n",
    "\n",
    "least_damaging = max(removal_impact.items(), key=lambda x: x[1]['impact'])\n",
    "print(f\"\\nüí° CANDIDATE: {least_damaging[0]}\")\n",
    "print(f\"    Score WITHOUT it: {least_damaging[1]['score']:.4f}\")\n",
    "print(f\"    Impact: {least_damaging[1]['impact']:+.4f}\")\n",
    "\n",
    "if least_damaging[1]['impact'] >= 0:\n",
    "    print(f\"    ‚úì Rimuoverlo MIGLIORA o non peggiora il modello!\")\n",
    "else:\n",
    "    print(f\"    ‚ö†Ô∏è  Rimuoverlo peggiora il modello di {abs(least_damaging[1]['impact']):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# METODO 4: FORWARD SELECTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METHOD 4: FORWARD SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort models based on individual performance\n",
    "sorted_models = sorted(individual_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Start with the best one\n",
    "selected = [sorted_models[0][0]]\n",
    "remaining = [m[0] for m in sorted_models[1:]]\n",
    "\n",
    "print(f\"üìå START WITH: {selected[0]} (Accuracy OOF: {sorted_models[0][1]:.4f})\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "forward_history = []\n",
    "current_score = cross_val_score(\n",
    "    meta_model, X_meta_df[selected], y_train, cv=kfold, scoring='accuracy', n_jobs=-1\n",
    ").mean()\n",
    "forward_history.append({'models': selected.copy(), 'score': current_score})\n",
    "\n",
    "print(f\"CV Score con [{', '.join(selected)}]: {current_score:.4f}\\n\")\n",
    "\n",
    "# Add model\n",
    "while remaining:\n",
    "    best_addition = None\n",
    "    best_score_so_far = current_score\n",
    "    \n",
    "    for candidate in remaining:\n",
    "        test_set = selected + [candidate]\n",
    "        score = cross_val_score(\n",
    "            meta_model, X_meta_df[test_set], y_train, cv=kfold, scoring='accuracy', n_jobs=-1\n",
    "        ).mean()\n",
    "        \n",
    "        if score > best_score_so_far:\n",
    "            best_score_so_far = score\n",
    "            best_addition = candidate\n",
    "    \n",
    "    if best_addition is not None:\n",
    "        selected.append(best_addition)\n",
    "        remaining.remove(best_addition)\n",
    "        improvement = best_score_so_far - current_score\n",
    "        current_score = best_score_so_far\n",
    "        forward_history.append({'models': selected.copy(), 'score': current_score})\n",
    "        \n",
    "        print(f\"‚ûï Added {best_addition:10s}: {current_score:.4f} (Œî = +{improvement:.4f})\")\n",
    "    else:\n",
    "        print(f\"\\n‚õî STOP: Nessun modello migliora ulteriormente il CV score\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüèÜ MIGLIOR COMBINAZIONE (Forward): {selected}\")\n",
    "print(f\"    CV Score: {current_score:.4f}\")\n",
    "\n",
    "# Plot history\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "scores = [h['score'] for h in forward_history]\n",
    "labels = [f\"{i+1}: {', '.join(h['models'][:2])}...\" if len(h['models']) > 2 \n",
    "          else f\"{i+1}: {', '.join(h['models'])}\" \n",
    "          for i, h in enumerate(forward_history)]\n",
    "ax.plot(range(1, len(scores)+1), scores, marker='o', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Numero di Modelli nell\\'Ensemble')\n",
    "ax.set_ylabel('CV Accuracy')\n",
    "ax.set_title('Forward Selection: Andamento CV Score')\n",
    "ax.set_xticks(range(1, len(scores)+1))\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.axhline(baseline_score, color='red', linestyle='--', label=f'Baseline (tutti): {baseline_score:.4f}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"v6_forward_selection.png\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 5: TRY ALL POSSIBLE COMBINATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METODO 5: VALUTAZIONE TUTTE LE COMBINAZIONI\")\n",
    "print(\"=\"*80)\n",
    "print(f\"(Con {len(X_meta_df.columns)} modelli ci sono {2**len(X_meta_df.columns)-1} combinazioni possibili)\\n\")\n",
    "\n",
    "all_models = list(X_meta_df.columns)\n",
    "all_combinations = []\n",
    "\n",
    "# Try all combination\n",
    "for size in range(1, len(all_models) + 1):\n",
    "    print(f\"Test combinazioni da {size} modelli...\")\n",
    "    for combo in combinations(all_models, size):\n",
    "        score = cross_val_score(\n",
    "            meta_model, X_meta_df[list(combo)], y_train, cv=kfold, scoring='accuracy', n_jobs=-1\n",
    "        ).mean()\n",
    "        all_combinations.append({\n",
    "            'models': list(combo),\n",
    "            'n_models': len(combo),\n",
    "            'score': score\n",
    "        })\n",
    "# Sort by score\n",
    "all_combinations_sorted = sorted(all_combinations, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "print(\"\\nüèÜ TOP 10 COMBINAZIONI:\")\n",
    "print(\"-\" * 80)\n",
    "for i, combo in enumerate(all_combinations_sorted[:10], 1):\n",
    "    models_str = ', '.join(combo['models'])\n",
    "    print(f\"{i:2d}. [{combo['n_models']} modelli] {combo['score']:.4f} - {models_str}\")\n",
    "\n",
    "# Best number of models\n",
    "print(\"\\nüìä MIGLIOR COMBINAZIONE PER NUMERO DI MODELLI:\")\n",
    "print(\"-\" * 80)\n",
    "best_by_size = {}\n",
    "for combo in all_combinations:\n",
    "    size = combo['n_models']\n",
    "    if size not in best_by_size or combo['score'] > best_by_size[size]['score']:\n",
    "        best_by_size[size] = combo\n",
    "\n",
    "for size in sorted(best_by_size.keys()):\n",
    "    combo = best_by_size[size]\n",
    "    models_str = ', '.join(combo['models'])\n",
    "    print(f\"{size} modelli: {combo['score']:.4f} - [{models_str}]\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for size in sorted(best_by_size.keys()):\n",
    "    scores_for_size = [c['score'] for c in all_combinations if c['n_models'] == size]\n",
    "    ax.scatter([size] * len(scores_for_size), scores_for_size, alpha=0.3, s=50)\n",
    "\n",
    "# Best ones\n",
    "best_scores = [best_by_size[s]['score'] for s in sorted(best_by_size.keys())]\n",
    "ax.plot(sorted(best_by_size.keys()), best_scores, 'ro-', linewidth=2, markersize=10, label='Best per size')\n",
    "ax.axhline(baseline_score, color='green', linestyle='--', linewidth=2, label=f'Baseline (tutti): {baseline_score:.4f}')\n",
    "ax.set_xlabel('Numero di Modelli nell\\'Ensemble')\n",
    "ax.set_ylabel('CV Accuracy')\n",
    "ax.set_title('Tutte le Combinazioni: CV Score vs Numero di Modelli')\n",
    "ax.set_xticks(sorted(best_by_size.keys()))\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"v6_all_combinations.png\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL REPORT\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã REPORT AND RECOMMENDETION (v6)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  BASELINE (All the {len(X_meta_df.columns)} models):\")\n",
    "print(f\"    CV Score: {baseline_score:.4f}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  BACKWARD ELIMINATION suggests:\")\n",
    "print(f\"    Remove: {least_damaging[0]}\")\n",
    "print(f\"    Score: {least_damaging[1]['score']:.4f}\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  FORWARD SELECTION suggests:\")\n",
    "print(f\"    Models: {selected}\")\n",
    "print(f\"    Score: {current_score:.4f}\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  BEST COMBINATION (Brute Force):\")\n",
    "best_overall = all_combinations_sorted[0]\n",
    "print(f\"    Models: {best_overall['models']}\")\n",
    "print(f\"    Score: {best_overall['score']:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "improvement = best_overall['score'] - baseline_score\n",
    "if improvement > 0.0005:  # Good improvement\n",
    "    print(f\"\\n‚úÖ RACCOMANDAZIONE: Usa la combinazione ottimale trovata\")\n",
    "    print(f\"    Miglioramento: +{improvement:.4f}\")\n",
    "    print(f\"    Modelli da usare: {best_overall['models']}\")\n",
    "elif improvement < -0.0005:  # Worsening\n",
    "    print(f\"\\n‚ö†Ô∏è  RACCOMANDAZIONE: Mantieni tutti i modelli (baseline)\")\n",
    "    print(f\"    La combinazione ottimale √® peggiore: {improvement:.4f}\")\n",
    "else:  # Negligible difference\n",
    "    print(f\"\\n‚û°Ô∏è  RACCOMANDAZIONE: Baseline vs Ottimale sono equivalenti\")\n",
    "    print(f\"    Differenza trascurabile: {improvement:.4f}\")\n",
    "    if len(best_overall['models']) < len(X_meta_df.columns):\n",
    "        print(f\"    Suggerisco: Usa {best_overall['models']} (pi√π semplice)\")\n",
    "    else:\n",
    "        print(f\"    Suggerisco: Mantieni tutti (pi√π robusto)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° SUGGERIMENTI AGGIUNTIVI:\")\n",
    "print(\"=\"*80)\n",
    "print(\"- Se hai modelli con correlazione >0.95, considera di rimuovere il pi√π debole\")\n",
    "print(\"- XGBoost e CatBoost tendono ad essere correlati (entrambi gradient boosting)\")\n",
    "print(\"- Il meta-modello (LogReg) pu√≤ dare pesi diversi ai modelli automaticamente\")\n",
    "print(\"- Se il dataset √® piccolo, meno modelli = meno overfitting del meta-modello\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE OPTIMAL SUBSET\n",
    "# ============================================================================\n",
    "print(f\"\\nüîß Creazione X_meta_df ottimizzato...\")\n",
    "optimal_models = best_overall['models']\n",
    "X_meta_df_optimal = X_meta_df[optimal_models].copy()\n",
    "print(f\"    X_meta_df_optimal creato con modelli: {optimal_models}\")\n",
    "print(f\"    Usa 'X_meta_df_optimal' nella cella successiva per il meta-modello finale\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61f107",
   "metadata": {},
   "source": [
    "Saving for the Ensemble Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08caf21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"--- Saving results ---\")\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# 2. Save just base optimal models\n",
    "\n",
    "print(f\"Salvataggio dei {len(optimal_models)} modelli base ottimali in /models/...\")\n",
    "\n",
    "for name in optimal_models:\n",
    "    if name in final_base_models:\n",
    "        model = final_base_models[name]\n",
    "        SAVE_PATH = f'../models/{name}.joblib'\n",
    "        joblib.dump(model, SAVE_PATH)\n",
    "        print(f\"Salvato: {SAVE_PATH}\")\n",
    "    else:\n",
    "        print(f\"ATTENZIONE: Modello {name} non trovato in final_base_models.\")\n",
    "\n",
    "# 3. Saving optimal meta features and target\n",
    "\n",
    "print(\"\\nSalvataggio delle meta-features e target per il meta-modello...\")\n",
    "\n",
    "META_OPTIMAL_PATH = '../data/processed/meta_features_optimal_train.csv'\n",
    "TARGET_PATH = '../data/processed/train_target.csv'\n",
    "\n",
    "X_meta_df_optimal.to_csv(META_OPTIMAL_PATH, index=False)\n",
    "y_train.to_csv(TARGET_PATH, index=False)\n",
    "\n",
    "print(f\"Salvate meta-features ottimali: {META_OPTIMAL_PATH}\")\n",
    "print(f\"Salvato target: {TARGET_PATH}\")\n",
    "\n",
    "print(\"\\n--- Salvataggio completato! ---\")\n",
    "print(\"Ora sei pronto per creare il notebook 07_Ensemble_Submission.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
